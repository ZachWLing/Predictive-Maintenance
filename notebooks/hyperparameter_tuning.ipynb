{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df3f872",
   "metadata": {},
   "source": [
    "# 1. Download and Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4776967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-tuner) (21.3)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-tuner) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging->keras-tuner) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->keras-tuner) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->keras-tuner) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->keras-tuner) (2.1.1)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a70075f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::bleach==5.0.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pytest==7.2.0=pyhd8ed1ab_2\n",
      "  - conda-forge/noarch::python-lsp-jsonrpc==1.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::qtpy==2.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::sip==6.7.5=py310hd8f1fbe_0\n",
      "  - conda-forge/noarch::terminado==0.17.1=pyh41d4057_0\n",
      "  - conda-forge/linux-64::watchdog==2.2.1=py310hff52083_0\n",
      "  - conda-forge/noarch::dask-core==2022.11.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::flask==2.2.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::importlib_metadata==6.0.0=hd8ed1ab_0\n",
      "  - conda-forge/noarch::nltk==3.8.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pyqt5-sip==12.11.0=py310hd8f1fbe_2\n",
      "  - conda-forge/noarch::python-lsp-server-base==1.7.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pytoolconfig==1.2.4=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::qdarkstyle==3.0.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::qtawesome==1.2.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::flask-cors==3.0.10=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::rope==1.6.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::keyring==23.13.1=py310hff52083_0\n",
      "  - conda-forge/noarch::nbformat==5.7.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-lsp-server==1.7.0=hd8ed1ab_0\n",
      "  - conda-forge/noarch::distributed==2022.11.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::ipykernel==6.20.1=pyh210e3f2_0\n",
      "  - intel/linux-64::mkl_fft==1.3.1=py310hcab1719_22\n",
      "  - intel/linux-64::mkl_random==1.2.2=py310hbf47bc3_22\n",
      "  - intel/linux-64::mkl_umath==0.1.1=py310hf66a691_32\n",
      "  - conda-forge/noarch::nbclient==0.7.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pyls-spyder==0.4.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pyqt==5.15.7=py310h29803b5_2\n",
      "  - conda-forge/noarch::python-lsp-black==1.2.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::requests==2.28.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::cookiecutter==2.1.1=pyh6c4a22f_0\n",
      "  - conda-forge/noarch::jupyter_console==6.4.4=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbconvert-core==7.2.7=pyhd8ed1ab_0\n",
      "  - intel/linux-64::numpy==1.22.3=py310hf0956d0_5\n",
      "  - conda-forge/noarch::pooch==1.6.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pyqtwebengine==5.15.7=py310h29803b5_2\n",
      "  - conda-forge/noarch::qtconsole-base==5.4.0=pyha770c72_0\n",
      "  - conda-forge/linux-64::requests-kerberos==0.12.0=py310hff52083_4\n",
      "  - conda-forge/noarch::s3transfer==0.6.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::spyder-kernels==2.4.1=unix_pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::bottleneck==1.3.5=py310hde88566_1\n",
      "  - conda-forge/linux-64::contourpy==1.0.6=py310hbf28c38_0\n",
      "  - conda-forge/linux-64::h5py==3.7.0=nompi_py310h416281c_102\n",
      "  - conda-forge/linux-64::imagecodecs==2022.12.24=py310h17758e3_0\n",
      "  - conda-forge/noarch::nbconvert-pandoc==7.2.7=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::numba==0.56.4=py310ha5257ce_0\n",
      "  - conda-forge/linux-64::numexpr==2.7.3=py310hb5077e9_1\n",
      "  - conda-forge/noarch::numpydoc==1.5.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pyerfa==2.0.0.1=py310hde88566_3\n",
      "  - conda-forge/linux-64::pywavelets==1.4.1=py310h0a54255_0\n",
      "  - conda-forge/noarch::qtconsole==5.4.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::sphinxcontrib-websupport==1.2.4=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::astropy==5.2=py310h0a54255_0\n",
      "  - conda-forge/noarch::jupyterlab_server==2.18.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbconvert==7.2.7=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::notebook-shim==0.2.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::patsy==0.5.3=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pytables==3.7.0=py310hb60b9b2_3\n",
      "  - conda-forge/noarch::tifffile==2022.10.10=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::dask==2022.11.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbclassic==0.4.8=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::scikit-image==0.19.3=py310h769672d_2\n",
      "  - conda-forge/linux-64::spyder==5.4.1=py310hff52083_1\n",
      "  - conda-forge/linux-64::statsmodels==0.13.5=py310hde88566_2\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py310hff52083_8\n",
      "  - conda-forge/noarch::hdijupyterutils==0.20.3=pyh1a96a4e_0\n",
      "  - conda-forge/noarch::autovizwidget==0.20.3=pyh1a96a4e_0\n",
      "  - conda-forge/noarch::sparkmagic==0.20.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::sphinx==5.1.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::bokeh==2.4.3=pyhd8ed1ab_3\n",
      "  - conda-forge/noarch::imageio==2.16.2=pyhcf75d05_0\n",
      "  - conda-forge/linux-64::matplotlib-base==3.5.3=py310h8d5ebf3_2\n",
      "  - conda-forge/linux-64::pandas==1.4.4=py310h769672d_0\n",
      "  - conda-forge/linux-64::scipy==1.8.1=py310hdfbd76f_3\n",
      "  - conda-forge/linux-64::matplotlib==3.5.3=py310hff52083_2\n",
      "  - conda-forge/noarch::seaborn-base==0.11.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::notebook==6.4.12=pyha770c72_0\n",
      "  - conda-forge/noarch::jupyterlab==3.3.4=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::seaborn==0.11.2=hd8ed1ab_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/tensorflow2_p310\n",
      "\n",
      "  added / updated specs:\n",
      "    - librosa\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    audioread-3.0.0            |  py310hff52083_1          34 KB  conda-forge\n",
      "    botocore-1.29.137          |     pyhd8ed1ab_0         5.8 MB  conda-forge\n",
      "    ca-certificates-2023.5.7   |       hbcca054_0         145 KB  conda-forge\n",
      "    certifi-2023.5.7           |     pyhd8ed1ab_0         149 KB  conda-forge\n",
      "    cloudpickle-2.2.1          |     pyhd8ed1ab_0          27 KB  conda-forge\n",
      "    ffmpeg-4.4                 |       h3fd9d12_2         9.7 MB  intel\n",
      "    jupyter_server-1.23.6      |     pyhd8ed1ab_0         238 KB  conda-forge\n",
      "    jupyterlab_widgets-3.0.7   |     pyhd8ed1ab_1         169 KB  conda-forge\n",
      "    lazy_loader-0.2            |     pyhd8ed1ab_0          13 KB  conda-forge\n",
      "    librosa-0.10.0             |     pyhd8ed1ab_2         188 KB  conda-forge\n",
      "    libsndfile-1.2.0           |       hb75c966_0         342 KB  conda-forge\n",
      "    openh264-2.1.0             |       hd408876_0         1.5 MB  intel\n",
      "    pulseaudio-16.1            |       ha8d29e2_1         1.5 MB  conda-forge\n",
      "    pysoundfile-0.12.1         |     pyhd8ed1ab_0          27 KB  conda-forge\n",
      "    scikit-learn-1.2.2         |  py310h41b6a48_1         7.3 MB  conda-forge\n",
      "    soxr-0.1.3                 |       h0b41bf4_3         128 KB  conda-forge\n",
      "    soxr-python-0.3.5          |  py310h278f3c1_0         259 KB  conda-forge\n",
      "    tqdm-4.65.0                |     pyhd8ed1ab_1          86 KB  conda-forge\n",
      "    ujson-5.7.0                |  py310heca2aa9_0          50 KB  conda-forge\n",
      "    websocket-client-1.5.2     |     pyhd8ed1ab_0          44 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        27.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  audioread          conda-forge/linux-64::audioread-3.0.0-py310hff52083_1 None\n",
      "  botocore           conda-forge/noarch::botocore-1.29.137-pyhd8ed1ab_0 None\n",
      "  cloudpickle        conda-forge/noarch::cloudpickle-2.2.1-pyhd8ed1ab_0 None\n",
      "  ffmpeg             intel/linux-64::ffmpeg-4.4-h3fd9d12_2 None\n",
      "  importlib-metadata conda-forge/noarch::importlib-metadata-6.6.0-pyha770c72_0 None\n",
      "  ipywidgets         conda-forge/noarch::ipywidgets-8.0.6-pyhd8ed1ab_0 None\n",
      "  jsonschema         conda-forge/noarch::jsonschema-4.17.3-pyhd8ed1ab_0 None\n",
      "  jupyter_client     conda-forge/noarch::jupyter_client-7.3.4-pyhd8ed1ab_0 None\n",
      "  jupyter_server     conda-forge/noarch::jupyter_server-1.23.6-pyhd8ed1ab_0 None\n",
      "  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-3.0.7-pyhd8ed1ab_1 None\n",
      "  lazy_loader        conda-forge/noarch::lazy_loader-0.2-pyhd8ed1ab_0 None\n",
      "  librosa            conda-forge/noarch::librosa-0.10.0-pyhd8ed1ab_2 None\n",
      "  nest-asyncio       conda-forge/noarch::nest-asyncio-1.5.6-pyhd8ed1ab_0 None\n",
      "  numpy-base         intel/linux-64::numpy-base-1.22.3-py310h45c9ace_5 None\n",
      "  openh264           intel/linux-64::openh264-2.1.0-hd408876_0 None\n",
      "  packaging          conda-forge/noarch::packaging-23.1-pyhd8ed1ab_0 None\n",
      "  pysoundfile        conda-forge/noarch::pysoundfile-0.12.1-pyhd8ed1ab_0 None\n",
      "  pyyaml             conda-forge/linux-64::pyyaml-6.0-py310h5764c6d_5 None\n",
      "  scikit-learn       conda-forge/linux-64::scikit-learn-1.2.2-py310h41b6a48_1 None\n",
      "  soxr               conda-forge/linux-64::soxr-0.1.3-h0b41bf4_3 None\n",
      "  soxr-python        conda-forge/linux-64::soxr-python-0.3.5-py310h278f3c1_0 None\n",
      "  tornado            conda-forge/linux-64::tornado-6.1-py310h5764c6d_3 None\n",
      "  tqdm               conda-forge/noarch::tqdm-4.65.0-pyhd8ed1ab_1 None\n",
      "  ujson              conda-forge/linux-64::ujson-5.7.0-py310heca2aa9_0 None\n",
      "  urllib3            conda-forge/noarch::urllib3-1.26.15-pyhd8ed1ab_0 None\n",
      "  websocket-client   conda-forge/noarch::websocket-client-1.5.2-pyhd8ed1ab_0 None\n",
      "  widgetsnbextension conda-forge/noarch::widgetsnbextension-4.0.7-pyhd8ed1ab_0 None\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2022.12.7-ha878542_0 --> 2023.5.7-hbcca054_0 None\n",
      "  certifi                            2022.12.7-pyhd8ed1ab_0 --> 2023.5.7-pyhd8ed1ab_0 None\n",
      "  importlib_metadata                       6.0.0-hd8ed1ab_0 --> 6.6.0-hd8ed1ab_0 None\n",
      "  libsndfile                               1.1.0-hcb278e6_1 --> 1.2.0-hb75c966_0 None\n",
      "  openssl                                  3.0.7-h0b41bf4_1 --> 3.1.0-hd590300_3 None\n",
      "  plotly                                5.11.0-pyhd8ed1ab_1 --> 5.14.1-pyhd8ed1ab_0 None\n",
      "  pulseaudio                                16.1-h126f2b6_0 --> 16.1-ha8d29e2_1 None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "websocket-client-1.5 | 44 KB     | ##################################### | 100% \n",
      "soxr-python-0.3.5    | 259 KB    | ##################################### | 100% \n",
      "scikit-learn-1.2.2   | 7.3 MB    | ##################################### | 100% \n",
      "tqdm-4.65.0          | 86 KB     | ##################################### | 100% \n",
      "librosa-0.10.0       | 188 KB    | ##################################### | 100% \n",
      "openh264-2.1.0       | 1.5 MB    | ##################################### | 100% \n",
      "ca-certificates-2023 | 145 KB    | ##################################### | 100% \n",
      "pysoundfile-0.12.1   | 27 KB     | ##################################### | 100% \n",
      "botocore-1.29.137    | 5.8 MB    | ##################################### | 100% \n",
      "ffmpeg-4.4           | 9.7 MB    | ##################################### | 100% \n",
      "ujson-5.7.0          | 50 KB     | ##################################### | 100% \n",
      "audioread-3.0.0      | 34 KB     | ##################################### | 100% \n",
      "jupyter_server-1.23. | 238 KB    | ##################################### | 100% \n",
      "libsndfile-1.2.0     | 342 KB    | ##################################### | 100% \n",
      "jupyterlab_widgets-3 | 169 KB    | ##################################### | 100% \n",
      "certifi-2023.5.7     | 149 KB    | ##################################### | 100% \n",
      "pulseaudio-16.1      | 1.5 MB    | ##################################### | 100% \n",
      "soxr-0.1.3           | 128 KB    | ##################################### | 100% \n",
      "cloudpickle-2.2.1    | 27 KB     | ##################################### | 100% \n",
      "lazy_loader-0.2      | 13 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc2dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import boto3\n",
    "import pickle\n",
    "import keras_tuner\n",
    "import numpy as np\n",
    "import librosa as lr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8d511",
   "metadata": {},
   "source": [
    "# 2. Import Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c5b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_pkl(bucket, key):\n",
    "    try:\n",
    "        loaded_pickle = pickle.loads(s3.Bucket(bucket).Object(key).get()['Body'].read())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return loaded_pickle['X_train'], loaded_pickle['y_train'], loaded_pickle['X_test'], loaded_pickle['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b3fb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = 'pumpaudio-zach'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64441130",
   "metadata": {},
   "outputs": [],
   "source": [
    "train00_X, train00_y, test00_X, test00_y = import_pkl(bucket, 'pump_raw/id_00.pkl')\n",
    "train02_X, train02_y, test02_X, test02_y = import_pkl(bucket, 'pump_raw/id_02.pkl')\n",
    "train04_X, train04_y, test04_X, test04_y = import_pkl(bucket, 'pump_raw/id_04.pkl')\n",
    "train06_X, train06_y, test06_X, test06_y = import_pkl(bucket, 'pump_raw/id_06.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea06dd",
   "metadata": {},
   "source": [
    "# 3. Define Model Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cb1d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    \n",
    "    def __init__(self, min_delta, patience):\n",
    "        self.input_shape_x = 0\n",
    "        self.input_shape_y = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.patience = patience \n",
    "    \n",
    "    def build(self, hp):\n",
    "        \n",
    "        n_mels = hp.Choice('n_mels', [64, 128, 256])\n",
    "        hop_length = hp.Choice('hop_length', [128, 256, 512])\n",
    "\n",
    "        self.input_shape_x = n_mels\n",
    "        self.input_shape_y = hop_length\n",
    "               \n",
    "        inputLayer = Input(shape = (self.input_shape_x, math.ceil((22050 * 10) / self.input_shape_y)))\n",
    "        h = Flatten()(inputLayer)\n",
    "        h = Dense(units = hp.Choice('layer 1', [64, 128, 256]), activation = 'relu')(h)\n",
    "        h = Dense(units = hp.Choice('layer 2', [64, 32, 16]), activation = 'relu')(h)\n",
    "        h = Dense(units = hp.Choice('layer 3', [4, 8]), activation = 'relu')(h)\n",
    "        h = Dense(units = hp.Choice('layer 4', [64, 32, 16]), activation = 'relu')(h)\n",
    "        h = Dense(units = hp.Choice('layer 5', [64, 128, 256]), activation = 'relu')(h)\n",
    "        h = Dense(self.input_shape_x * math.ceil((22050 * 10) / self.input_shape_y), activation = None)(h)\n",
    "        h = Reshape((self.input_shape_x, math.ceil((22050 * 10) / self.input_shape_y)))(h)\n",
    "                  \n",
    "        Model(inputs = inputLayer, outputs = h).compile(loss = 'mean_squared_error', metrics = ['mse'])\n",
    "\n",
    "        return Model(inputs = inputLayer, outputs = h)\n",
    "\n",
    "    def fit(self, hp, model, train, validation, callbacks = None, **kwargs):\n",
    "        \n",
    "        def convert_to_melspectrogram(audio_files, sr, n_fft, hop_length, n_mels):\n",
    "            \n",
    "            audio_mel = lr.feature.melspectrogram(y = audio_files[0], sr = sr, n_fft = n_fft, hop_length = hop_length, n_mels = n_mels)\n",
    "            audio_mel_dbd = lr.amplitude_to_db(abs(audio_mel), ref = np.max)\n",
    "            shape_0 = audio_mel_dbd.shape[0]\n",
    "            shape_1 = audio_mel_dbd.shape[1]\n",
    "            audio_mel_dbd = np.reshape(audio_mel_dbd, [1, shape_0, shape_1])\n",
    "\n",
    "            for file in audio_files[1:]:\n",
    "                audio_mel_temp = lr.feature.melspectrogram(y = file, sr = sr, n_fft = n_fft, hop_length = hop_length, n_mels = n_mels)\n",
    "                audio_mel_dbd_temp = lr.amplitude_to_db(abs(audio_mel_temp), ref = np.max)\n",
    "                audio_mel_dbd = np.vstack([audio_mel_dbd, np.reshape(audio_mel_dbd_temp, [1, shape_0, shape_1])])\n",
    "            return audio_mel_dbd\n",
    "\n",
    "        train_ds = convert_to_melspectrogram(train, 22050, 1024, self.input_shape_y, self.input_shape_x)\n",
    "        batch_size = hp.Choice(\"batch_size\", [256, 512])\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices(train_ds).batch(batch_size)\n",
    "        \n",
    "        # Define the optimizer.\n",
    "        optimizer = keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling = 'log', default = 1e-3))\n",
    "        loss_fn = keras.losses.MeanSquaredError(reduction = 'auto', name = 'mean_squared_error')\n",
    "\n",
    "        # The metric to track validation loss.\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "\n",
    "        # Function to run the train step.\n",
    "        @tf.function\n",
    "        def run_train_step(train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(train)\n",
    "                loss = loss_fn(train, logits)\n",
    "                # Add any regularization losses.\n",
    "                if model.losses:\n",
    "                    loss +=  tf.math.add_n(model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Function to run the validation step.\n",
    "        @tf.function\n",
    "        def run_val_step(validation):\n",
    "            logits = model(validation)\n",
    "            loss = loss_fn(validation, logits)\n",
    "            # Update the metric.\n",
    "            epoch_loss_metric.update_state(loss)\n",
    "\n",
    "        # Assign the model to the callbacks.\n",
    "        for callback in callbacks:\n",
    "            callback.model = model\n",
    "\n",
    "        # Record the best validation loss value\n",
    "        best_epoch_loss = float(\"inf\")\n",
    "\n",
    "        # The custom training loop.\n",
    "        epochs = hp.Choice('epochs', [50, 75, 100])\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "\n",
    "            # Iterate the training data to run the training step.\n",
    "            for sample in train_ds:\n",
    "                sample_temp = sample.numpy()\n",
    "                run_train_step(sample_temp)\n",
    "\n",
    "            # Iterate the validation data to run the validation step.\n",
    "            for sample in train_ds:\n",
    "                sample_temp = sample.numpy()\n",
    "                run_val_step(sample_temp)\n",
    "\n",
    "            # Calling the callbacks after epoch.\n",
    "            epoch_loss = float(epoch_loss_metric.result().numpy())\n",
    "            for callback in callbacks:\n",
    "                # The \"my_metric\" is the objective passed to the tuner.\n",
    "                callback.on_epoch_end(epoch, logs = {\"my_metric\": epoch_loss})\n",
    "            epoch_loss_metric.reset_states()\n",
    "\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "            \n",
    "            if best_epoch_loss <= (epoch_loss + self.min_delta):\n",
    "                self.wait += 1\n",
    "                if self.wait == self.patience:\n",
    "                    break\n",
    "            else:\n",
    "                best_epoch_loss = min(best_epoch_loss, epoch_loss)\n",
    "                self.wait = 0\n",
    "\n",
    "        # Return the evaluation metric value.\n",
    "        return best_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d942b4",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a3ed9",
   "metadata": {},
   "source": [
    "### Model 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99597437",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_00 = keras_tuner.RandomSearch(\n",
    "    objective = keras_tuner.Objective('my_metric', 'min'),\n",
    "    seed = 41,\n",
    "    max_trials = 60,\n",
    "    hypermodel = MyHyperModel(min_delta = 1, patience = 2),\n",
    "    directory = '../model/results',\n",
    "    project_name = 'custom_training',\n",
    "    overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eefa1a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 05m 50s]\n",
      "my_metric: 80.3096923828125\n",
      "\n",
      "Best my_metric So Far: 51.71058654785156\n",
      "Total elapsed time: 03h 03m 41s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_00.search(train = train00_X, validation = train00_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d10ed451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_mels': 64, 'hop_length': 128, 'layer 1': 128, 'layer 2': 32, 'layer 3': 4, 'layer 4': 16, 'layer 5': 128, 'batch_size': 256, 'learning_rate': 0.00010674541740374034, 'epochs': 50}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 1723)]        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 110272)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               14114944  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               2176      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 110272)            14225088  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 64, 1723)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,346,548\n",
      "Trainable params: 28,346,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_hps_00 = tuner_00.get_best_hyperparameters()[0]\n",
    "print(best_hps_00.values)\n",
    "\n",
    "best_model_00 = tuner_00.get_best_models()[0]\n",
    "print(best_model_00.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200eb0d",
   "metadata": {},
   "source": [
    "### Model 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "238bb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_02 = keras_tuner.RandomSearch(\n",
    "    objective = keras_tuner.Objective('my_metric', 'min'),\n",
    "    seed = 41,\n",
    "    max_trials = 60,\n",
    "    hypermodel = MyHyperModel(min_delta = 1, patience = 2),\n",
    "    directory = '../model/results',\n",
    "    project_name = 'custom_training',\n",
    "    overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f49f20ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 04m 42s]\n",
      "my_metric: 203.61766052246094\n",
      "\n",
      "Best my_metric So Far: 46.55274963378906\n",
      "Total elapsed time: 02h 33m 11s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_02.search(train = train02_X, validation = train02_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8afce29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_mels': 64, 'hop_length': 256, 'layer 1': 64, 'layer 2': 32, 'layer 3': 4, 'layer 4': 64, 'layer 5': 128, 'batch_size': 256, 'learning_rate': 0.00021217076286104294, 'epochs': 50}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 862)]         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 55168)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3530816   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                320       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 55168)             7116672   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 64, 862)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,658,340\n",
      "Trainable params: 10,658,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_hps_02 = tuner_02.get_best_hyperparameters()[0]\n",
    "print(best_hps_02.values)\n",
    "\n",
    "best_model_02 = tuner_02.get_best_models()[0]\n",
    "print(best_model_02.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac3e6d",
   "metadata": {},
   "source": [
    "### Model 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcadb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_04 = keras_tuner.RandomSearch(\n",
    "    objective = keras_tuner.Objective('my_metric', 'min'),\n",
    "    seed = 41,\n",
    "    max_trials = 60,\n",
    "    hypermodel = MyHyperModel(min_delta = 1, patience = 2),\n",
    "    directory = '../model/results',\n",
    "    project_name = 'custom_training',\n",
    "    overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e9fad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 02m 28s]\n",
      "my_metric: 3413.7060546875\n",
      "\n",
      "Best my_metric So Far: 43.1404914855957\n",
      "Total elapsed time: 01h 22m 22s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_04.search(train = train04_X, validation = train04_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f039c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_mels': 64, 'hop_length': 256, 'layer 1': 64, 'layer 2': 64, 'layer 3': 8, 'layer 4': 16, 'layer 5': 64, 'batch_size': 256, 'learning_rate': 0.003378131836989523, 'epochs': 75}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 862)]         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 55168)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3530816   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                144       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 55168)             3585920   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 64, 862)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,122,648\n",
      "Trainable params: 7,122,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_hps_04 = tuner_04.get_best_hyperparameters()[0]\n",
    "print(best_hps_04.values)\n",
    "\n",
    "best_model_04 = tuner_04.get_best_models()[0]\n",
    "print(best_model_04.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ae4a7",
   "metadata": {},
   "source": [
    "### Model 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd29f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_06 = keras_tuner.RandomSearch(\n",
    "    objective = keras_tuner.Objective('my_metric', 'min'),\n",
    "    seed = 41,\n",
    "    max_trials = 60,\n",
    "    hypermodel = MyHyperModel(min_delta = 1, patience = 2),\n",
    "    directory = '../model/results',\n",
    "    project_name = 'custom_training',\n",
    "    overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e1adca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 05m 06s]\n",
      "my_metric: 105.09736633300781\n",
      "\n",
      "Best my_metric So Far: 37.78456497192383\n",
      "Total elapsed time: 02h 48m 03s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_06.search(train = train06_X, validation = train06_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "915c3a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_mels': 64, 'hop_length': 256, 'layer 1': 128, 'layer 2': 32, 'layer 3': 8, 'layer 4': 64, 'layer 5': 64, 'batch_size': 256, 'learning_rate': 0.00011307631534561521, 'epochs': 100}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 862)]         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 55168)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               7061632   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 55168)             3585920   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 64, 862)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,656,680\n",
      "Trainable params: 10,656,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_hps_06 = tuner_06.get_best_hyperparameters()[0]\n",
    "print(best_hps_06.values)\n",
    "\n",
    "best_model_06 = tuner_06.get_best_models()[0]\n",
    "print(best_model_06.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd35af",
   "metadata": {},
   "source": [
    "# 4. Write Best Models to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb7562bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_pickle (bucket, key, model):\n",
    "    pickle_obj = pickle.dumps(model) \n",
    "    s3.Bucket(bucket).Object(key).put(Body = pickle_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c8181bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......reshape\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-05-22 13:42:36           64\n",
      "config.json                                    2023-05-22 13:42:36         3753\n",
      "variables.h5                                   2023-05-22 13:42:36    113410704\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......reshape\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-05-22 13:42:38           64\n",
      "config.json                                    2023-05-22 13:42:38         3748\n",
      "variables.h5                                   2023-05-22 13:42:38     42659216\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......reshape\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-05-22 13:42:38           64\n",
      "config.json                                    2023-05-22 13:42:38         3747\n",
      "variables.h5                                   2023-05-22 13:42:38     28515752\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......flatten\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......reshape\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2023-05-22 13:42:39           64\n",
      "config.json                                    2023-05-22 13:42:39         3748\n",
      "variables.h5                                   2023-05-22 13:42:39     42652824\n"
     ]
    }
   ],
   "source": [
    "dump_pickle(bucket, 'models/model_00.pkl', best_model_00)\n",
    "dump_pickle(bucket, 'models/model_02.pkl', best_model_02)\n",
    "dump_pickle(bucket, 'models/model_04.pkl', best_model_04)\n",
    "dump_pickle(bucket, 'models/model_06.pkl', best_model_06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c833b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
